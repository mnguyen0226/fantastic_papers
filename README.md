# Lists of Trustworthy ML Techniques

## 1. Poisoning Attacks
- "Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks" - Shafahi et al. NeurIPS 2018. 
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/Poison%20Frogs!%20Targeted%20Clean-Label%20Poisoning%20Attacks%20on%20Neural%20Networks.pdf)
    - [Code](https://github.com/ashafahi/inceptionv3-transferLearn-poison)
- "Understanding Black-box Predictions via Influence Functions" - Koh et al. ICML 2017. 
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/Understanding%20Black-box%20Predictions%20via%20Influence%20Functions.pdf)
    - [Code 1](https://worksheets.codalab.org/worksheets/0x2b314dc3536b482dbba02783a24719fd/), [Code 2](https://worksheets.codalab.org/worksheets/0x2b314dc3536b482dbba02783a24719fd/)

## 2. Backdoor Attacks
- "BadNets: Identifying Vulnerabilities in Machine Learning Model Supply Chain" - Gu et al.
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/BadNets:%20Identifying%20Vulnerabilities%20in%20the%20Machine%20Learning%20Model%20Supply%20Chain.pdf)
    - Code
- "Rethinking the Backdoor Attacks's Triggers: A Frequency Perspective" - Zeng et al. ICCV 2021
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/Rethinking%20the%20Backdoor%20Attacks%E2%80%99%20Triggers:%20A%20Frequency%20Perspective.pdf)
    - Code
- "Transferable Clean-Label Poisoning Attacks on Deep Neural Nets" - Zhu et al. 2019
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/Transferable%20Clean-Label%20Poisoning%20Attacks%20on%20Deep%20Neural%20Nets.pdf)
    - Code
- "Poisoning Attacks against Support Vector Machines" - Biggio et al. 2013
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/Poisoning%20Attacks%20against%20Support%20Vector%20Machines.pdf)
    - Code
- "Efficient Label Contamination Attacks Against Black-Box Learning Models" - Zhao et al. 2017
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/Efficient%20Label%20Contamination%20Attacks%20Against%20Black-Box%20Learning%20Models.pdf)
    - Code
- "Trojaning Attack on Neural Networks" - Liu et al. 2017
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/Trojaning%20Attack%20on%20Neural%20Networks%20.pdf)
    - Code
- "Label-Consistent Backdoor Attacks" - Turner et al. 2019
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/Label-Consistent%20Backdoor%20Attacks.pdf)
    - Code
- "Latent Backdoor Attacks on Deep Neural Networks" - Yao et al. 2019
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/Latent%20Backdoor%20Attacks%20on%20Deep%20Neural%20Networks.pdf)
    - Code
- "BAAAN: Backdoor Attacks Against Autoencoder and GAN-Based Machine Learning Models" - Salem et al. 2020
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/BAAAN:%20Backdoor%20Attacks%20Against%20Autoencoder%20and%20GAN-Based.pdf)
    - Code

## 3. Training-time Defense
- "Deep Partition Aggregation: Provable Defense against General Poisoning Attacks" - Levine et al. 2021
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/Deep%20Partition%20Aggregation:%20Provable%20Defense%20against%20General%20Poisoning%20Attacks.pdf)
    - Code
- "DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations" - Borgnia et al. 2021
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/DP-InstaHide:%20Provably%20Defusing%20Poisoning%20and%20Backdoor%20Attacks%20with%20DP-InstaHide:%20Provably%20Defusing%20Poisoning%20and%20Backdoor%20Attacks%20with%20Differentially%20Private%20Data%20Augmentations.pdf)
    - Code
- "On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping" - Hong et al. 2020
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/On%20the%20Effectiveness%20of%20Mitigating%20Data%20Poisoning%20Attacks%20with%20Gradient%20Shaping.pdf.pdf)
    - Code
- "Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks" - Wang et al. 2019
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/Neural%20Cleanse:%20Identifying%20and%20Mitigating%20Backdoor%20Attacks%20in%20Neural%20Networks.pdf)
    - Code
- "Adversarial Unlearning of Backdoors via Implicit Hypergradient" - Zeng et al. 2022
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/Adversarial%20Unlearning%20of%20Backdoors%20via%20Implicit%20Hypergradient.pdf)
    - Code

## 4. Data Quality

## 5. Data Valuation

## 6. Privacy Attacks

## 7. Differential Privacy

## 8. Differentially Private Learning 

## 9. Robust Estimation

## 10. Evasion Attack

## 11. Test-time Defenses

## 12. Ceertified Defense

## 13. Federated Learning

## 14. Federated Learning Security

## 15. Federated Learning Privacy

## 16. Uncertainty Calibration