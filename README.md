# Fantastic Papers :page_facing_up: and Where to Find Them :sparkles:

## :four_leaf_clover: Graph Neural Networks
### 1. Architectures
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

### 2. Evaluations
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Sequential Models
### 1. Architectures
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

### 2. Evaluations
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Synthetic Time-Series Data Generation

### 1. Architectures
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Lin et al. | [Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions](https://dl.acm.org/doi/abs/10.1145/3419394.3423643) | ACM 2020 |  |  |  |  |
Yoon et al. | [Time-series Generative Adversarial Networks](https://proceedings.neurips.cc/paper/2019/hash/c9efe5f26cd17ba6216bbe2a7d26d490-Abstract.html) | NeurIPS 2019 |  |  |  |  |
Alaa et al. | [Generative Time-series Modeling with Fourier Flows](https://openreview.net/forum?id=PpshD0AXfA) | ICLR 2021 |  |  |  |  |
Jeha et al. | [PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series](https://openreview.net/forum?id=Ix_mh42xq5w) | ICLR 2022 |  |  |  |  |
Fawaz et al. | [Data augmentation using synthetic data for time series classification with deep residual networks](https://arxiv.org/abs/1808.02455) |  ArXiv 2018 |  |  |  |  |
Desai et al. | [TimeVAE: A Variational Auto-Encoder for Multivariate Time Series Generation](https://arxiv.org/abs/2111.08095) | ArXiv 2021 |  |  |  |  |
Esteban et al. | [Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs](https://arxiv.org/abs/1706.02633) | ArXiv 2017 |  |  |  |  |
Che et al. | [Recurrent Neural Networks for Multivariate Time Series with Missing Values](https://www.nature.com/articles/s41598-018-24271-9) | Nature 2018 |  |  |  |  |
Ramponi et al. | [T-CGAN: Conditional Generative Adversarial Network for Data Augmentation in Noisy Time Series with Irregular Sampling](https://arxiv.org/abs/1811.08295) |ArXiv 2018  |  |  |  |  |
Tadayon et al. | [tsBNgen: A Python Library to Generate Time Series Data from an Arbitrary Dynamic Bayesian Network Structure](https://arxiv.org/abs/2009.04595) | ArXiv 2020 |  |  |  |  |
Ni et al. | [Conditional Sig-Wasserstein GANs for Time Series Generation](https://arxiv.org/abs/2006.05421) | ArXiv 2020 |  |  |  |  |
Pinceti et al. | [Synthetic Time-Series Load Data via Conditional Generative Adversarial Networks](https://ieeexplore.ieee.org/abstract/document/9637821) | IEEE 2021 |  |  |  |  |
Lawrence et al. | [Data Generating Process to Evaluate Causal Discovery Techniques for Time Series Data](https://arxiv.org/abs/2104.08043) | ArXiv 2021 |  |  |  |  |
Perez et al. | [Data augmentation through multivariate scenario forecasting in Data Centers using Generative Adversarial Networks](https://arxiv.org/abs/2201.06147) | ArXiv 2022 |  |  |  |  |
Derek Snow | [MTSS-GAN: Multivariate Time Series Simulation Generative Adversarial Networks](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3616557) | SSRN 2020 |  |  |  |  |
Smith et al. | [Conditional GAN for timeseries generation](https://arxiv.org/abs/2006.16477) | ArXiv 2020 |  |  |  |  |
Pinceti et al. | [Synthetic Time-Series Load Data via Conditional Generative Adversarial Networks](https://ieeexplore.ieee.org/abstract/document/9637821) | IEEE 2021 |  |  |  |  |
Xu et al. | [STAN: Synthetic Network Traffic Generation with Generative Neural Models](https://link.springer.com/chapter/10.1007/978-3-030-87839-9_1) | Springer 2021 |  |  |  |  |
Leznik et al. | [Multivariate Time Series Synthesis Using Generative Adversarial Networks](https://dl.acm.org/doi/abs/10.1145/3427921.3450257) | ACM 2021 |  |  |  |  |
Forestier et al. | [Generating Synthetic Time Series to Augment Sparse Datasets](https://ieeexplore.ieee.org/abstract/document/8215569?casa_token=PUSMjBd7Wy4AAAAA:bofU2I-usgEYd9P2CdOXHcc0Gt5dRLw6VGKQaHHJ2tRj8IZvXVZ5wLeU75ZA6Iy76r94wYFqKQ) | IEEE 2017 |  |  |  |  |


### 2. Evaluations
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
UCR Lab | [Matrix Profile](https://www.cs.ucr.edu/~eamonn/MatrixProfile.html) |  |  |  |  |  |
Fauvel et al. | [XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification](https://www.mdpi.com/2227-7390/9/23/3137) | MDPI 2021 |  |  |  |  |
Ismail et al. | [Benchmarking Deep Learning Interpretability in Time Series Predictions](https://proceedings.neurips.cc/paper/2020/hash/47a3893cc405396a5c30d91320572d6d-Abstract.html) | NeurIPS 2020 |  |  |  |  |
Agarwal et al. | [Model Agnostic Time Series Analysis via Matrix Estimation](https://dl.acm.org/doi/abs/10.1145/3287319?casa_token=t8Goi7ihzuEAAAAA:BxECAdjUmrbQC42WTRKXYPO6aK55JSKsufoC7eZjR-Fv_W1zcctW1M61E2Hwhz5hTWxkMHLObRLb) | ACM 2018 |  |  |  |  |
Harutyunyan et al. | [Efficient Covariance Estimation from Temporal Data](https://arxiv.org/abs/1905.13276) | ArXiv 2019 |  |  |  |  |

## :four_leaf_clover: Trustworthy Machine Learning 
### 1. Training-time Attacks - Poisoning Attacks

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Solans et al. | [Poisoning Attacks on Algorithmic Fairness](https://link.springer.com/chapter/10.1007/978-3-030-67658-2_10) | Springer 2020 |  |  |  |  |
Li et al. | [Data poisoning attacks on factorization-based collaborative filtering](https://proceedings.neurips.cc/paper/2016/hash/83fa5a432ae55c253d0e60dbfa716723-Abstract.html) | NeurIPS 2016 |  |  |  |  |
Huang et al. | [Metapoison: Practical general-purpose clean-label data poisoning](https://proceedings.neurips.cc/paper/2020/hash/8ce6fc704072e351679ac97d4a985574-Abstract.html) | NeurIPS 2020 |  |  |  |  |
Shan et al. | [Fawkes: Protecting Privacy against Unauthorized Deep Learning Models](https://www.usenix.org/conference/usenixsecurity20/presentation/shan) | USENIX Security Symposium 2020 |  |  |  |  |
Shafahi et al. | [Poison frogs! targeted clean-label poisoning attacks on neural networks](https://proceedings.neurips.cc/paper/2018/hash/22722a343513ed45f14905eb07621686-Abstract.html) | NeurIPS 2018 |  |  |  |  |
Zhu et al. | [Transferable Clean-Label Poisoning Attacks on Deep Neural Nets](http://proceedings.mlr.press/v97/zhu19a.html) | PMLR 2019 |  |  |  |  |
Schioppa et al. | [Scaling Up Influence Functions](https://arxiv.org/abs/2112.03052) | ArXiv 2021 |  |  |  |  |
Biggio et al. | [Poisoning Attacks against Support Vector Machines](https://arxiv.org/abs/1206.6389) | ArXiv 2012 |  |  |  |  |
Zhao et al. | [Efficient Label Contamination Attacks Against Black-Box Learning Models](https://www.ijcai.org/Proceedings/2017/0551.pdf) | IJCAI 2017 |  |  |  |  |
Koh et al. | [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730) | ArXiv 2017 |  |  |  |  |
Yao et al. | [Latent Backdoor Attacks on Deep Neural Networks](https://dl.acm.org/doi/abs/10.1145/3319535.3354209) | ACM CCS 2019 |  |  |  |  |

### 2. Training-time Attacks - Backdoor Attacks

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Turner et al. | [Label-Consistent Backdoor Attacks](https://arxiv.org/abs/1912.02771) | ArXiv 2019 |  |  |  |  |
Chen et al. | [Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning](https://arxiv.org/abs/1712.05526) | ArXiv 2017 |  |  |  |  |
Gu et al. | [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733) | ArXiv 2019 |  |  |  |  |
Wallace et al. | [Imitation attacks and defenses for black-box machine translation systems](https://arxiv.org/abs/2004.15015) | ArXiv 2020 |  |  |  |  |
Salem et al. | [BAAAN: Backdoor Attacks Against Autoencoder and GAN-Based Machine Learning Models](https://arxiv.org/abs/2010.03007) | ArXiV 2020 |  |  |  |  |
Wang et al. | [A Trigger Exploration Method for Backdoor Attacks on Deep Learning-Based Traffic Control Systems](https://ieeexplore.ieee.org/abstract/document/9683577?casa_token=MCrgmABy7E8AAAAA:14E5T_NDRbJMAA9QnJhfNuby8sRojqqjy-0CnQ0rT7oeBrIB2GaNGeewwJ5rjrG8QqkEh1IzBw) | IEEE 2021 |  |  |  |  |
Adi et al. | [Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring](https://www.usenix.org/conference/usenixsecurity18/presentation/adi) | USENIX Security Symposium 2018 |  |  |  |  |
Liu et al. | [Trojaning Attack on Neural Networks](https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=2782&context=cstech) | Purdue University 2017 |  |  |  |  |
Turner et al. | [Metropolis-Hastings Generative Adversarial Networks](https://proceedings.mlr.press/v97/turner19a.html) | PMLR 2019 |  |  |  |  |
Saha et al. | [Hidden Trigger Backdoor Attacks](https://ojs.aaai.org/index.php/AAAI/article/view/6871) | AAAI 2020 |  |  |  |  |

### 3. Training-time Data Poisoning Defenses

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Wang et al | [Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks](https://ieeexplore.ieee.org/abstract/document/8835365?casa_token=czPHUrjctSgAAAAA:8HwtzpqgYvZ8lwtJAEltcf1YQ2A5rDB0fycsCRkv5JyAW8I03f0TyIgeH_0t2F9-uJkrrsEkKw) | IEEE 2019 |  |  |  |  |
Chen et al. | [DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks](http://www.aceslab.org/sites/default/files/DeepInspect.pdf) | IJCAI 2019 |  |  |  |  |
Guo et al. | [TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems](https://arxiv.org/abs/1908.01763) | ArXiV 2019 |  |  |  |  |
Chen et al. | [Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting](https://arxiv.org/abs/2004.12651) | ArXiv 2020 |  |  |  |  |
Zeng et al. | [Adversarial Unlearning of Backdoors via Implicit Hypergradient](https://openreview.net/forum?id=MeeQkFYVbzW) | ICLR 2022 |  |  |  |  |
Hong et al. | [On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping](https://arxiv.org/abs/2002.11497) | ArXiv 2020 |  |  |  |  |
Ma et al. | [Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics](https://ieeexplore.ieee.org/abstract/document/8812988?casa_token=Fsw5s8_xaYEAAAAA:AK1kks88Bj-xRJn1Xq6aL4IIWxrF1DtuYCb-FaEAMpnMSPJDcH2274DgH4Y2AZZ5OhZZLQ6JBA) | IEEE 2019 |  |  |  |  |
Borgnia et al. | [Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff](https://ieeexplore.ieee.org/abstract/document/9414862?casa_token=lERRhmQ15WQAAAAA:VBQJNGUHR6bTSN6xy46bS8kKkm2iTAIJWgU22QmWTq3fCJ1XG023wCksKybEnsOtyud2UXO4QA) | IEEE 2021 |  |  |  |  |
Borgnia et al. | [DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations](https://arxiv.org/abs/2103.02079) | ArXiv 2021 |  |  |  |  |

### 4. Data Quality, Data Valuation, and Poisoned Data Detection

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Jia et al. | [Towards Efficient Data Valuation Based on the Shapley Value](https://proceedings.mlr.press/v89/jia19a.html) | PMLR 2019 |  |  |  |  |
Jia et al. | [Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms](https://arxiv.org/abs/1908.08619) | VLDB 2019 |  |  |  |  |
Ghorbani et al. | [Data Shapley: Equitable Valuation of Data for Machine Learning](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4390) | ICML 2019 |  |  |  |  |
Yoon et al. | [Data Valuation using Reinforcement Learning](http://proceedings.mlr.press/v119/yoon20a.html) | PMLR 2020 |  |  |  |  |
Jia et al. | [Scalability vs. Utility: Do We Have To Sacrifice One for the Other in Data Importance Quantification?](https://openaccess.thecvf.com/content/CVPR2021/html/Jia_Scalability_vs._Utility_Do_We_Have_To_Sacrifice_One_for_CVPR_2021_paper.html) | CVPR 2021 |  |  |  |  |
Koh et al. | [Stronger data poisoning attacks break data sanitization defenses](https://link.springer.com/article/10.1007/s10994-021-06119-y) | Springer 2021 |  |  |  |  |
Chen et al. | [Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering](https://arxiv.org/abs/1811.03728) | ArXiv 2018 |  |  |  |  |
Peri et al. | [Deep k-NN Defense Against Clean-Label Data Poisoning Attacks](https://link.springer.com/chapter/10.1007/978-3-030-66415-2_4) | Springer 2021 |  |  |  |  |
Gao et al. | [STRIP: a defence against trojan attacks on deep neural networks](https://dl.acm.org/doi/abs/10.1145/3359789.3359790?casa_token=IqbL9HJjReAAAAAA:sj_EbbUKG-JMbZh3VclMVzeNl_4R0P2dNOORg0CB268HGH1NPDxq3RAS8qg4NUiolSKXKimtkCMI) | ACM 2019 |  |  |  |  |
Chou et al. | [SentiNet: Detecting Localized Universal Attacks Against Deep Learning Systems](https://arxiv.org/abs/1812.00292) | ArXiv 2018 |  |  |  |  |

### 5. Robust Mean Estimation in High Dimensions

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Zico Kolter and Aleksander Madry | [Adversarial Robustness - Theory and Practice](https://adversarial-ml-tutorial.org/) | NeurIPS 2018 |  |  |  |  |
Jerry Li | [Principled Approaches to Robust Machine Learning and Beyond](https://jerryzli.github.io/papers/phd-thesis.pdf) | MIT's Ph.D Thesis 2018 |  |  |  |  |
Jacob Steinhardt | [Robust Learning: Information Theory and Algorithms](https://www.stat.berkeley.edu/~jsteinhardt/pubs/steinhardt2018thesis.pdf) | Stanford's Ph.D Thesis 2018 |  |  |  |  |
Zhu et al. | [When does the Tukey Median work?](https://ieeexplore.ieee.org/abstract/document/9173995?casa_token=xAEyUler0lUAAAAA:c5jKyb5HnzURk8GKGU66h4jg_L_Az8ZwVg7lpxWd6mtrfnwGrASSqhNLvkMQ00WIdC0CqI0_-g) | IEEE 2020 |  |  |  |  |
Diakonikolas et al. | [Sever: A Robust Meta-Algorithm for Stochastic Optimization](https://proceedings.mlr.press/v97/diakonikolas19a.html) | PMLR 2019 |  |  |  |  |
Tran et al. | [Spectral Signatures in Backdoor Attacks](https://proceedings.neurips.cc/paper/2018/hash/280cf18baf4311c92aa5a042336587d3-Abstract.html) | NeurIPS 2018 |  |  |  |  |
Abbe et al. | [Poly-time universality and limitations of deep learning](https://arxiv.org/abs/2001.02992) | ArXiv 2020 |  |  |  |  |

### 6. Privacy Attacks - Poisoned Data Detection

*Awesome ML Privacy Attacks Deep-dive: [Github](https://github.com/stratosphereips/awesome-ml-privacy-attacks).

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Fredrikson et al. | [Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing](https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/fredrikson_matthew) | USENIX Security Symposium 2014 |  |  |  |  |
Fredrikson et al. | [Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | ACM 2015 |  |  |  |  |
Zhang & Jia et al. | [The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_The_Secret_Revealer_Generative_Model-Inversion_Attacks_Against_Deep_Neural_Networks_CVPR_2020_paper.html) | CVPR 2020 |  |  |  |  |
Yang et al. | [Adversarial Neural Network Inversion via Auxiliary Knowledge Alignment](https://arxiv.org/abs/1902.08552) | ArXiv 2019 |  |  |  |  |
Shokri et al. | [Membership Inference Attacks Against Machine Learning Models](https://arxiv.org/abs/1610.05820) | ArXiv 2016 |  |  |  |  |
Nasr et al. | [Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning](https://ieeexplore.ieee.org/abstract/document/8835245?casa_token=xEUKdV-QynIAAAAA:JKKNSAymt3rR6vcBBnG1mAeyXXjZEd6wjhwtYEygP2wK4_1ukP8m18oPKU2cER8Bk3IjobwBPA) | IEEE 2019 |  |  |  |  |
Carlini et al. | [The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks](https://www.usenix.org/conference/usenixsecurity19/presentation/carlini) | USENIX Security Symposium 2019 |  |  |  |  |
Carlini et al. | [Extracting Training Data from Large Language Models](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting) | USENIX Security Symposium 2022 |  |  |  |  |

### 7. Differential Privacy

*Awesome ML Differential Privacy Deep-dive: [Github](https://github.com/Billy1900/Awesome-Differential-Privacy).

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Aaron Roth and Cynthia Dwork | [The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf) | University of Pennsylvania's Book |  |  |  |  |
Abadi et al. | [Deep Learning with Differential Privacy](https://dl.acm.org/doi/abs/10.1145/2976749.2978318?casa_token=04AV26T6VT4AAAAA:LjK85DNm0rJ-lk0S2VDwtmAnHuIBVyKMYeU89wucbz6abzM8tDygrgWjI1ZzdQNUEvcKpW4nEMeb) | ACM 2016 |  |  |  |  |
Papernot et al. | [Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data](https://arxiv.org/abs/1610.05755) | ArXiv 2016 |  |  |  |  |
Papernot et al. | [Scalable Private Learning with PATE](https://arxiv.org/abs/1802.08908) | ArXiv 2018 |  |  |  |  |

### 8. Federated Learning

*Awesome ML Federated Learning Deep-dive: [Github](https://github.com/poga/awesome-federated-learning).

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
McMahan et al. | [Communication-Efficient Learning of Deep Networks from Decentralized Data](http://proceedings.mlr.press/v54/mcmahan17a.html) | PMLR 2017 |  |  |  |  |
Konečný et al. | [Federated Optimization:Distributed Optimization Beyond the Datacenter](https://arxiv.org/abs/1511.03575) | ArXiv 2015 |  |  |  |  |
Stich et al. | [Local SGD Converges Fast and Communicates Little](https://arxiv.org/abs/1805.09767) | ICLR 2018 |  |  |  |  |
Li et al. | [On the Convergence of FedAvg on Non-IID Data](https://arxiv.org/abs/1907.02189) | ICLR 2020 |  |  |  |  |
Shamir et al. | [Communication-Efficient Distributed Optimization using an Approximate Newton-type Method](https://proceedings.mlr.press/v32/shamir14.html) | PMLR 2014 |  |  |  |  |
Wang et al. | [GIANT: Globally Improved Approximate Newton Method for Distributed Optimization](https://proceedings.neurips.cc/paper/2018/hash/dabd8d2ce74e782c65a973ef76fd540b-Abstract.html) | NeurIPS 2018 |  |  |  |  |
Mahajan et al. | [An efficient distributed learning algorithm based on effective local functional approximations](https://arxiv.org/abs/1310.8418) | ArXiv 2013 |  |  |  |  |
Smith et al. | [CoCoA: A General Framework for Communication-Efficient Distributed Optimization](https://www.jmlr.org/papers/volume18/16-512/16-512.pdf) | JMLR 2018 |  |  |  |  |
Zhang et al. | [Communication-Efficient Algorithms for Statistical Optimization](https://proceedings.neurips.cc/paper/2012/hash/e7f8a7fb0b77bcb3b283af5be021448f-Abstract.html) | NeurIPS 2013 |  |  |  |  |
Geiping et al. | [Inverting Gradients - How easy is it to break privacy in federated learning?](https://proceedings.neurips.cc/paper/2020/hash/c4ede56bbd98819ae6112b20ac6bf145-Abstract.html) | NeurIPS 2020 |  |  |  |  |
Agarwal et al. | [cpSGD: Communication-efficient and differentially-private distributed SGD](https://proceedings.neurips.cc/paper/2018/hash/21ce689121e39821d07d04faab328370-Abstract.html) | NeurIPS 2018 |  |  |  |  |
Bagdasaryan et al. | [How To Backdoor Federated Learning](https://proceedings.mlr.press/v108/bagdasaryan20a.html) | PMLR 2020 |  |  |  |  |
Bhagoji et al. | [Analyzing Federated Learning through an Adversarial Lens](https://proceedings.mlr.press/v97/bhagoji19a.html) | PMLR 2019 |  |  |  |  |
Tolpegin et al. | [Data Poisoning Attacks Against Federated Learning Systems](https://link.springer.com/chapter/10.1007/978-3-030-58951-6_24) | Springer 2020 |  |  |  |  |
Sun et al. | [Data Poisoning Attacks on Federated Machine Learning](https://ieeexplore.ieee.org/abstract/document/9618642?casa_token=nLXXx0riOtoAAAAA:gBTZfA-5ZvPikVGI-ksdtlYAbT3d2-WzCeyNTB086f_jHqV0rJVU9l2NrJPFJeUDltA2XbZzFw) | IEEE 2021 |  |  |  |  |
Fung et al. | [Mitigating Sybils in Federated Learning Poisoning](https://arxiv.org/abs/1808.04866) | ArXiv 2018 |  |  |  |  |
Li et al. | [Learning to Detect Malicious Clients for Robust Federated Learning](https://arxiv.org/abs/2002.00211) | ArXiv 2020 |  |  |  |  |
Blanchard et al. | [Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent](https://proceedings.neurips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html) | NeurIPS 2017 |  |  |  |  |
Mhamdi et al. | [The Hidden Vulnerability of Distributed Learning in Byzantium](https://proceedings.mlr.press/v80/mhamdi18a.html) | PMLR 2018 |  |  |  |  |
Yin et al. | [Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates](https://proceedings.mlr.press/v80/yin18a) | PMLR 2018 |  |  |  |  |
Chen et al. | [Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent](https://arxiv.org/abs/1705.05491) | ArXiv 2017 |  |  |  |  |
Fu et al. | [Attack-Resistant Federated Learning with Residual-based Reweighting](https://arxiv.org/abs/1912.11464) | ArXiv 2019 |  |  |  |  |
Sun et al. | [Can You Really Backdoor Federated Learning?](https://arxiv.org/abs/1911.07963) | ArXiv 2019 |  |  |  |  |
Andreina et al. | [BaFFLe: Backdoor Detection via Feedback-based Federated Learning](https://ieeexplore.ieee.org/abstract/document/9546463?casa_token=rTpaToMQfwQAAAAA:_w-HPkRDko2IciGmQM4XB31OVqgT-6mAYV6znvDhYZVZwpQYCg2333nCrqrV5WyMQ7KmeMV5rQ) | IEEE 2021 |  |  |  |  |
Wu et al. | [Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2011.01767) | ArXiv 2021 |  |  |  |  |

### 9. Evasion Attacks & Defenses

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Akhtar et al. | [Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey](https://ieeexplore.ieee.org/abstract/document/8294186) | IEEE 2018 |  |  |  |  |
Athalye et al. | [Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](http://proceedings.mlr.press/v80/athalye18a.html) | PMLR 2018 |  |  |  |  |
Dhillon et al. | [Stochastic Activation Pruning for Robust Adversarial Defense](https://arxiv.org/abs/1803.01442) | ArXiv 2018 |  |  |  |  |
Schmidt et al. | [Adversarially Robust Generalization Requires More Data](https://proceedings.neurips.cc/paper/2018/hash/f708f064faaf32a43e4d3c784e6af9ea-Abstract.html) | NeurIPS 2018 |  |  |  |  |
Yin et al. | [Rademacher Complexity for Adversarially Robust Generalization](http://proceedings.mlr.press/v97/yin19b.html) | PMLR 2019 |  |  |  |  |
Tsipras et al. | [Robustness May Be at Odds with Accuracy](https://arxiv.org/abs/1805.12152) | ArXiv 2018 |  |  |  |  |

### 10. Certified Defenses

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Lecuyer et al. | [Certified Robustness to Adversarial Examples with Differential Privacy](https://ieeexplore.ieee.org/abstract/document/8835364?casa_token=BgsxdAlduEgAAAAA:cQdNBInOvS-ZeMZcadgtG1e0pArr6R-OWBnX0aeyuRMbqjMBIvNuCnAcnBaRjnBybl-TEN86WA) | IEEE 2019 |  |  |  |  |
Li et al. | [Certified Adversarial Robustness with Additive Noise](https://proceedings.neurips.cc/paper/2019/hash/335cd1b90bfa4ee70b39d08a4ae0cf2d-Abstract.html) | NeurIPS 2019 |  |  |  |  |


### 11. Uncertainty Calibration

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Battleday et al. | [Improving machine classification using human uncertainty measurements](https://openreview.net/forum?id=rJl8BhRqF7) | ICLR 2019 |  |  |  |  |
Guo et al. | [On Calibration of Modern Neural Networks](http://proceedings.mlr.press/v70/guo17a.html) | PMLR 2017 |  |  |  |  |
Hendrycks et al. | [The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization](https://openaccess.thecvf.com/content/ICCV2021/html/Hendrycks_The_Many_Faces_of_Robustness_A_Critical_Analysis_of_Out-of-Distribution_ICCV_2021_paper.html) | ICCV 2021 |  |  |  |  |
Ren et al. | [Improving Out-of-Distribution Detection in Machine Learning Models](https://ai.googleblog.com/2019/12/improving-out-of-distribution-detection.html) | Google AI Blog |  |  |  |  |
Larson et al. | [An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction](https://arxiv.org/abs/1909.02027) | ArXiv 2019 |  |  |  |  |
Hendrycks et al. | [Benchmarking Neural Network Robustness to Common Corruptions and Perturbations](https://arxiv.org/abs/1903.12261) | ArXiv 2019 |  |  |  |  |
Nguyen et al. | [Deep Neural Networks Are Easily Fooled: High Confidence Predictions for Unrecognizable Images](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.html) | CVPR 2015 |  |  |  |  |
Liu et al. | [Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness](https://proceedings.neurips.cc/paper/2020/hash/543e83748234f7cbab21aa0ade66565f-Abstract.html) | NeurIPS 2020 |  |  |  |  |
Gal et al. | [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](http://proceedings.mlr.press/v48/gal16.html?ref=https://githubhelp.com) | PMLR 2016 |  |  |  |  |
Lakshminarayanan et al. | [Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles](https://arxiv.org/abs/1612.01474) | ArXiv 2016 |  |  |  |  |

### 12. AI Assurance

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Batarseh et al. | [A survey on artificial intelligence assurance](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00445-7) | Springer 2021 |  |  |  |  |

## :four_leaf_clover: Optimization Techniques

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Statistical Methods & Data Analytics 
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Reinforcement Learning
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Software Engineering

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## Other Lists:
- [Dartmouth Deep Learning Papers](https://www.cs.dartmouth.edu/~lorenzo/teaching/cs189/readinglist.html)
- [Deep Learning Reading List](http://jmozah.github.io/links/)
- [Neural Networks Paper](https://github.com/robertsdionne/neural-network-papers)
- [Deep Learning Bibliography](https://github.com/memkite/DeepLearningBibliography)
- [Deep Learning for NLP](https://github.com/andrewt3000/DL4NLP)
- [List of Speech and NLP](https://joshdotai.medium.com/a-curated-list-of-speech-and-natural-language-processing-resources-4d89f94c032a)
- [100 Must-read Papers on NLP](https://github.com/mhagiwara/100-nlp-papers)
- [Awesome RNN Papers](https://github.com/kjw0612/awesome-rnn)
- [Awesome Deep Learning Papers](https://github.com/ChristosChristofidis/awesome-deep-learning)
- [Lists of Optimization Techniques Papers](https://paperswithcode.com/methods/category/stochastic-optimization)
- [Awesome GAN](https://github.com/nightrome/really-awesome-gan)
- [A School for all Seasons on Trustworthy Machine Learning](https://trustworthy-machine-learning.github.io/)
- [Awesome Deep Trading](https://github.com/firmai/awesome-deep-trading)
- [Awesome AI in Finance](https://github.com/firmai/awesome-ai-in-finance)
- [Awesome Quant](https://github.com/firmai/awesome-quant)
- [List of Van Der Shaar Lab](https://github.com/firmai/tsgan)
- List of GNN: [Lectures](https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html), [Papers](https://pytorch-geometric.readthedocs.io/en/latest/notes/resources.html)
- [Must-read Papers on GNN](https://github.com/thunlp/GNNPapers)
- [Deep RL Papers](https://github.com/junhyukoh/deep-reinforcement-learning-papers)
- [Awesome RL Papers](https://github.com/aikorea/awesome-rl)