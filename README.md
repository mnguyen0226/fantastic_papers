# Fantastic Papers :page_facing_up: and Where to Find Them :sparkles:

## :four_leaf_clover: Deep Learning Fundamentals

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Synthetic Time-Series Data Generation

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Trustworthy Machine Learning 
### 1. Training-time Attacks - Poisoning Attacks

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Solans et al. | [Poisoning Attacks on Algorithmic Fairness](https://link.springer.com/chapter/10.1007/978-3-030-67658-2_10) | Springer 2020 |  |  |  |  |
Li et al. | [Data poisoning attacks on factorization-based collaborative filtering](https://proceedings.neurips.cc/paper/2016/hash/83fa5a432ae55c253d0e60dbfa716723-Abstract.html) | NeurIPS 2016 |  |  |  |  |
Huang et al. | [Metapoison: Practical general-purpose clean-label data poisoning](https://proceedings.neurips.cc/paper/2020/hash/8ce6fc704072e351679ac97d4a985574-Abstract.html) | NeurIPS 2020 |  |  |  |  |
Shan et al. | [Fawkes: Protecting Privacy against Unauthorized Deep Learning Models](https://www.usenix.org/conference/usenixsecurity20/presentation/shan) | USENIX Security Symposium 2020 |  |  |  |  |
Shafahi et al. | [Poison frogs! targeted clean-label poisoning attacks on neural networks](https://proceedings.neurips.cc/paper/2018/hash/22722a343513ed45f14905eb07621686-Abstract.html) | NeurIPS 2018 |  |  |  |  |
Zhu et al. | [Transferable Clean-Label Poisoning Attacks on Deep Neural Nets](http://proceedings.mlr.press/v97/zhu19a.html) | PMLR 2019 |  |  |  |  |
Schioppa et al. | [Scaling Up Influence Functions](https://arxiv.org/abs/2112.03052) | ArXiv 2021 |  |  |  |  |
Biggio et al. | [Poisoning Attacks against Support Vector Machines](https://arxiv.org/abs/1206.6389) | ArXiv 2012 |  |  |  |  |
Zhao et al. | [Efficient Label Contamination Attacks Against Black-Box Learning Models](https://www.ijcai.org/Proceedings/2017/0551.pdf) | IJCAI 2017 |  |  |  |  |
Koh et al. | [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730) | ArXiv 2017 |  |  |  |  |
Yao et al. | [Latent Backdoor Attacks on Deep Neural Networks](https://dl.acm.org/doi/abs/10.1145/3319535.3354209) | ACM CCS 2019 |  |  |  |  |

### 2. Training-time Attacks - Backdoor Attacks

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Turner et al. | [Label-Consistent Backdoor Attacks](https://arxiv.org/abs/1912.02771) | ArXiv 2019 |  |  |  |  |
Chen et al. | [Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning](https://arxiv.org/abs/1712.05526) | ArXiv 2017 |  |  |  |  |
Gu et al. | [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733) | ArXiv 2019 |  |  |  |  |
Wallace et al. | [Imitation attacks and defenses for black-box machine translation systems](https://arxiv.org/abs/2004.15015) | ArXiv 2020 |  |  |  |  |
Salem et al. | [BAAAN: Backdoor Attacks Against Autoencoder and GAN-Based Machine Learning Models](https://arxiv.org/abs/2010.03007) | ArXiV 2020 |  |  |  |  |
Wang et al. | [A Trigger Exploration Method for Backdoor Attacks on Deep Learning-Based Traffic Control Systems](https://ieeexplore.ieee.org/abstract/document/9683577?casa_token=MCrgmABy7E8AAAAA:14E5T_NDRbJMAA9QnJhfNuby8sRojqqjy-0CnQ0rT7oeBrIB2GaNGeewwJ5rjrG8QqkEh1IzBw) | IEEE 2021 |  |  |  |  |
Adi et al. | [Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring](https://www.usenix.org/conference/usenixsecurity18/presentation/adi) | USENIX Security Symposium 2018 |  |  |  |  |
Liu et al. | [Trojaning Attack on Neural Networks](https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=2782&context=cstech) | Purdue University 2017 |  |  |  |  |
Turner et al. | [Metropolis-Hastings Generative Adversarial Networks](https://proceedings.mlr.press/v97/turner19a.html) | PMLR 2019 |  |  |  |  |
Saha et al. | [Hidden Trigger Backdoor Attacks](https://ojs.aaai.org/index.php/AAAI/article/view/6871) | AAAI 2020 |  |  |  |  |

### 3. Training-time Data Poisoning Defenses

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Wang et al | [Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks](https://ieeexplore.ieee.org/abstract/document/8835365?casa_token=czPHUrjctSgAAAAA:8HwtzpqgYvZ8lwtJAEltcf1YQ2A5rDB0fycsCRkv5JyAW8I03f0TyIgeH_0t2F9-uJkrrsEkKw) | IEEE 2019 |  |  |  |  |
Chen et al. | [DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks](http://www.aceslab.org/sites/default/files/DeepInspect.pdf) | IJCAI 2019 |  |  |  |  |
Guo et al. | [TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems](https://arxiv.org/abs/1908.01763) | ArXiV 2019 |  |  |  |  |
Chen et al. | [Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting](https://arxiv.org/abs/2004.12651) | ArXiv 2020 |  |  |  |  |
Zeng et al. | [Adversarial Unlearning of Backdoors via Implicit Hypergradient](https://openreview.net/forum?id=MeeQkFYVbzW) | ICLR 2022 |  |  |  |  |
Hong et al. | [On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping](https://arxiv.org/abs/2002.11497) | ArXiv 2020 |  |  |  |  |
Ma et al. | [Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics](https://ieeexplore.ieee.org/abstract/document/8812988?casa_token=Fsw5s8_xaYEAAAAA:AK1kks88Bj-xRJn1Xq6aL4IIWxrF1DtuYCb-FaEAMpnMSPJDcH2274DgH4Y2AZZ5OhZZLQ6JBA) | IEEE 2019 |  |  |  |  |
Borgnia et al. | [Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff](https://ieeexplore.ieee.org/abstract/document/9414862?casa_token=lERRhmQ15WQAAAAA:VBQJNGUHR6bTSN6xy46bS8kKkm2iTAIJWgU22QmWTq3fCJ1XG023wCksKybEnsOtyud2UXO4QA) | IEEE 2021 |  |  |  |  |

### 4. Data Quality, Data Valuation, and Poisoned Data Detection

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Jia et al. | [Towards Efficient Data Valuation Based on the Shapley Value](https://proceedings.mlr.press/v89/jia19a.html) | PMLR 2019 |  |  |  |  |
Jia et al. | [Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms](https://arxiv.org/abs/1908.08619) | VLDB 2019 |  |  |  |  |
Ghorbani et al. | [Data Shapley: Equitable Valuation of Data for Machine Learning](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4390) | ICML 2019 |  |  |  |  |
Yoon et al. | [Data Valuation using Reinforcement Learning](http://proceedings.mlr.press/v119/yoon20a.html) | PMLR 2020 |  |  |  |  |
Jia et al. | [Scalability vs. Utility: Do We Have To Sacrifice One for the Other in Data Importance Quantification?](https://openaccess.thecvf.com/content/CVPR2021/html/Jia_Scalability_vs._Utility_Do_We_Have_To_Sacrifice_One_for_CVPR_2021_paper.html) | CVPR 2021 |  |  |  |  |
Koh et al. | [Stronger data poisoning attacks break data sanitization defenses](https://link.springer.com/article/10.1007/s10994-021-06119-y) | Springer 2021 |  |  |  |  |
Chen et al. | [Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering](https://arxiv.org/abs/1811.03728) | ArXiv 2018 |  |  |  |  |
Peri et al. | [Deep k-NN Defense Against Clean-Label Data Poisoning Attacks](https://link.springer.com/chapter/10.1007/978-3-030-66415-2_4) | Springer 2021 |  |  |  |  |
Gao et al. | [STRIP: a defence against trojan attacks on deep neural networks](https://dl.acm.org/doi/abs/10.1145/3359789.3359790?casa_token=IqbL9HJjReAAAAAA:sj_EbbUKG-JMbZh3VclMVzeNl_4R0P2dNOORg0CB268HGH1NPDxq3RAS8qg4NUiolSKXKimtkCMI) | ACM 2019 |  |  |  |  |
Chou et al. | [SentiNet: Detecting Localized Universal Attacks Against Deep Learning Systems](https://arxiv.org/abs/1812.00292) |  |  |  |  |  |

### 5. Privacy Attacks - Poisoned Data Detection

*Awesome ML Privacy Attacks Deep-dive: [Github](https://github.com/stratosphereips/awesome-ml-privacy-attacks).

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |
 |  |  |  |  |  |  |
 |  |  |  |  |  |  |
 |  |  |  |  |  |  |
 |  |  |  |  |  |  |
 |  |  |  |  |  |  |


### 6. Different Privacy

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Aaron Roth and Cynthia Dwork | [The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf) | University of Pennsylvania's Book |  |  |  |  |
 |  |  |  |  |  |  |
 |  |  |  |  |  |  |
 |  |  |  |  |  |  |

### 7. Robust Mean Estimation in High Dimensions

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Zico Kolter and Aleksander Madry | [Adversarial Robustness - Theory and Practice](https://adversarial-ml-tutorial.org/) | NeurIPS 2018 |  |  |  |  |
Jerry Li | [Principled Approaches to Robust Machine Learning and Beyond](https://jerryzli.github.io/papers/phd-thesis.pdf) | MIT's Ph.D Thesis 2018 |  |  |  |  |
Jacob Steinhardt | [Robust Learning: Information Theory and Algorithms](https://www.stat.berkeley.edu/~jsteinhardt/pubs/steinhardt2018thesis.pdf) | Stanford's Ph.D Thesis 2018 |  |  |  |  |
Zhu et al. | [When does the Tukey Median work?](https://ieeexplore.ieee.org/abstract/document/9173995?casa_token=xAEyUler0lUAAAAA:c5jKyb5HnzURk8GKGU66h4jg_L_Az8ZwVg7lpxWd6mtrfnwGrASSqhNLvkMQ00WIdC0CqI0_-g) | IEEE 2020 |  |  |  |  |
Diakonikolas et al. | [Sever: A Robust Meta-Algorithm for Stochastic Optimization](https://proceedings.mlr.press/v97/diakonikolas19a.html) | PMLR 2019 |  |  |  |  |
Tran et al. | [Spectral Signatures in Backdoor Attacks](https://proceedings.neurips.cc/paper/2018/hash/280cf18baf4311c92aa5a042336587d3-Abstract.html) | NeurIPS 2018 |  |  |  |  |
Abbe et al. | [Poly-time universality and limitations of deep learning](https://arxiv.org/abs/2001.02992) | ArXiv 2020 |  |  |  |  |

### 8. Evasion Attack

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

### 9. Test-Time Defenses

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

### 10. Certified Defenses

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

### 11. Federated Learning

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

### 12. Uncertainty Calibration

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

### 13. AI Assurance

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Batarseh et al. | [A survey on artificial intelligence assurance](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00445-7) | Springer 2021 |  |  |  |  |


## :four_leaf_clover: Statistical Methods & Data Analytics 
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Representative Learning
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Sequential Learning
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Reinforcement Learning
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Software Engineering

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |
