# Fantastic Papers :page_facing_up: and Where to Find Them :sparkles:

## :four_leaf_clover: Deep Learning Fundamentals

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Synthetic Time-Series Data Generation

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Trustworthy Machine Learning 
### 1. Training-time Attacks - Poisoning Attacks

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Solans et al. | [Poisoning Attacks on Algorithmic Fairness](https://link.springer.com/chapter/10.1007/978-3-030-67658-2_10) | Springer 2020 |  |  |  |  |
Li et al. | [Data poisoning attacks on factorization-based collaborative filtering](https://proceedings.neurips.cc/paper/2016/hash/83fa5a432ae55c253d0e60dbfa716723-Abstract.html) | NeurIPS 2016 |  |  |  |  |
Huang et al. | [Metapoison: Practical general-purpose clean-label data poisoning](https://proceedings.neurips.cc/paper/2020/hash/8ce6fc704072e351679ac97d4a985574-Abstract.html) | NeurIPS 2020 |  |  |  |  |
Shan et al. | [Fawkes: Protecting Privacy against Unauthorized Deep Learning Models](https://www.usenix.org/conference/usenixsecurity20/presentation/shan) | USENIX Security Symposium 2020 |  |  |  |  |
Shafahi et al. | [Poison frogs! targeted clean-label poisoning attacks on neural networks](https://proceedings.neurips.cc/paper/2018/hash/22722a343513ed45f14905eb07621686-Abstract.html) | NeurIPS 2018 |  |  |  |  |
Zhu et al. | [Transferable Clean-Label Poisoning Attacks on Deep Neural Nets](http://proceedings.mlr.press/v97/zhu19a.html) | PMLR 2019 |  |  |  |  |
Schioppa et al. | [Scaling Up Influence Functions](https://arxiv.org/abs/2112.03052) | ArXiv 2021 |  |  |  |  |
Biggio et al. | [Poisoning Attacks against Support Vector Machines](https://arxiv.org/abs/1206.6389) | ArXiv 2012 |  |  |  |  |
Zhao et al. | [Efficient Label Contamination Attacks Against Black-Box Learning Models](https://www.ijcai.org/Proceedings/2017/0551.pdf) | IJCAI 2017 |  |  |  |  |
Koh et al. | [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730) | ArXiv 2017 |  |  |  |  |
Yao et al. | [Latent Backdoor Attacks on Deep Neural Networks](https://dl.acm.org/doi/abs/10.1145/3319535.3354209) | ACM CCS 2019 |  |  |  |  |

### 2. Training-time Attacks - Backdoor Attacks

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Turner et al. | [Label-Consistent Backdoor Attacks](https://arxiv.org/abs/1912.02771) | ArXiv 2019 |  |  |  |  |
Chen et al. | [Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning](https://arxiv.org/abs/1712.05526) | ArXiv 2017 |  |  |  |  |
Gu et al. | [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733) | ArXiv 2019 |  |  |  |  |
Wallace et al. | [Imitation attacks and defenses for black-box machine translation systems](https://arxiv.org/abs/2004.15015) | ArXiv 2020 |  |  |  |  |
Salem et al. | [BAAAN: Backdoor Attacks Against Autoencoder and GAN-Based Machine Learning Models](https://arxiv.org/abs/2010.03007) | ArXiV 2020 |  |  |  |  |
Wang et al. | [A Trigger Exploration Method for Backdoor Attacks on Deep Learning-Based Traffic Control Systems](https://ieeexplore.ieee.org/abstract/document/9683577?casa_token=MCrgmABy7E8AAAAA:14E5T_NDRbJMAA9QnJhfNuby8sRojqqjy-0CnQ0rT7oeBrIB2GaNGeewwJ5rjrG8QqkEh1IzBw) | IEEE 2021 |  |  |  |  |
Adi et al. | [Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring](https://www.usenix.org/conference/usenixsecurity18/presentation/adi) | USENIX Security Symposium 2018 |  |  |  |  |
Liu et al. | [Trojaning Attack on Neural Networks](https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=2782&context=cstech) | Purdue University 2017 |  |  |  |  |
Turner et al. | [Metropolis-Hastings Generative Adversarial Networks](https://proceedings.mlr.press/v97/turner19a.html) | PMLR 2019 |  |  |  |  |
Saha et al. | [Hidden Trigger Backdoor Attacks](https://ojs.aaai.org/index.php/AAAI/article/view/6871) | AAAI 2020 |  |  |  |  |

### 3. Training-time Data Poisoning Defenses

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Wang et al | [Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks](https://ieeexplore.ieee.org/abstract/document/8835365?casa_token=czPHUrjctSgAAAAA:8HwtzpqgYvZ8lwtJAEltcf1YQ2A5rDB0fycsCRkv5JyAW8I03f0TyIgeH_0t2F9-uJkrrsEkKw) | IEEE 2019 |  |  |  |  |
Chen et al. | [DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks](http://www.aceslab.org/sites/default/files/DeepInspect.pdf) | IJCAI 2019 |  |  |  |  |
Guo et al. | [TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems](https://arxiv.org/abs/1908.01763) | ArXiV 2019 |  |  |  |  |
Chen et al. | [Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting](https://arxiv.org/abs/2004.12651) | ArXiv 2020 |  |  |  |  |
Zeng et al. | [Adversarial Unlearning of Backdoors via Implicit Hypergradient](https://openreview.net/forum?id=MeeQkFYVbzW) | ICLR 2022 |  |  |  |  |
Hong et al. | [On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping](https://arxiv.org/abs/2002.11497) | ArXiv 2020 |  |  |  |  |
Ma et al. | [Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics](https://ieeexplore.ieee.org/abstract/document/8812988?casa_token=Fsw5s8_xaYEAAAAA:AK1kks88Bj-xRJn1Xq6aL4IIWxrF1DtuYCb-FaEAMpnMSPJDcH2274DgH4Y2AZZ5OhZZLQ6JBA) | IEEE 2019 |  |  |  |  |
Borgnia et al. | [Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff](https://ieeexplore.ieee.org/abstract/document/9414862?casa_token=lERRhmQ15WQAAAAA:VBQJNGUHR6bTSN6xy46bS8kKkm2iTAIJWgU22QmWTq3fCJ1XG023wCksKybEnsOtyud2UXO4QA) | IEEE 2021 |  |  |  |  |
Borgnia et al. | [DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations](https://arxiv.org/abs/2103.02079) | ArXiv 2021 |  |  |  |  |

### 4. Data Quality, Data Valuation, and Poisoned Data Detection

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Jia et al. | [Towards Efficient Data Valuation Based on the Shapley Value](https://proceedings.mlr.press/v89/jia19a.html) | PMLR 2019 |  |  |  |  |
Jia et al. | [Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms](https://arxiv.org/abs/1908.08619) | VLDB 2019 |  |  |  |  |
Ghorbani et al. | [Data Shapley: Equitable Valuation of Data for Machine Learning](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4390) | ICML 2019 |  |  |  |  |
Yoon et al. | [Data Valuation using Reinforcement Learning](http://proceedings.mlr.press/v119/yoon20a.html) | PMLR 2020 |  |  |  |  |
Jia et al. | [Scalability vs. Utility: Do We Have To Sacrifice One for the Other in Data Importance Quantification?](https://openaccess.thecvf.com/content/CVPR2021/html/Jia_Scalability_vs._Utility_Do_We_Have_To_Sacrifice_One_for_CVPR_2021_paper.html) | CVPR 2021 |  |  |  |  |
Koh et al. | [Stronger data poisoning attacks break data sanitization defenses](https://link.springer.com/article/10.1007/s10994-021-06119-y) | Springer 2021 |  |  |  |  |
Chen et al. | [Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering](https://arxiv.org/abs/1811.03728) | ArXiv 2018 |  |  |  |  |
Peri et al. | [Deep k-NN Defense Against Clean-Label Data Poisoning Attacks](https://link.springer.com/chapter/10.1007/978-3-030-66415-2_4) | Springer 2021 |  |  |  |  |
Gao et al. | [STRIP: a defence against trojan attacks on deep neural networks](https://dl.acm.org/doi/abs/10.1145/3359789.3359790?casa_token=IqbL9HJjReAAAAAA:sj_EbbUKG-JMbZh3VclMVzeNl_4R0P2dNOORg0CB268HGH1NPDxq3RAS8qg4NUiolSKXKimtkCMI) | ACM 2019 |  |  |  |  |
Chou et al. | [SentiNet: Detecting Localized Universal Attacks Against Deep Learning Systems](https://arxiv.org/abs/1812.00292) | ArXiv 2018 |  |  |  |  |

### 5. Privacy Attacks - Poisoned Data Detection

*Awesome ML Privacy Attacks Deep-dive: [Github](https://github.com/stratosphereips/awesome-ml-privacy-attacks).

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Fredrikson et al. | [Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing](https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/fredrikson_matthew) | USENIX Security Symposium 2014 |  |  |  |  |
Fredrikson et al. | [Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | ACM 2015 |  |  |  |  |
Zhang & Jia et al. | [The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_The_Secret_Revealer_Generative_Model-Inversion_Attacks_Against_Deep_Neural_Networks_CVPR_2020_paper.html) | CVPR 2020 |  |  |  |  |
Yang et al. | [Adversarial Neural Network Inversion via Auxiliary Knowledge Alignment](https://arxiv.org/abs/1902.08552) | ArXiv 2019 |  |  |  |  |
Shokri et al. | [Membership Inference Attacks Against Machine Learning Models](https://arxiv.org/abs/1610.05820) | ArXiv 2016 |  |  |  |  |
Nasr et al. | [Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning](https://ieeexplore.ieee.org/abstract/document/8835245?casa_token=xEUKdV-QynIAAAAA:JKKNSAymt3rR6vcBBnG1mAeyXXjZEd6wjhwtYEygP2wK4_1ukP8m18oPKU2cER8Bk3IjobwBPA) | IEEE 2019 |  |  |  |  |
Carlini et al. | [The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks](https://www.usenix.org/conference/usenixsecurity19/presentation/carlini) | USENIX Security Symposium 2019 |  |  |  |  |
Carlini et al. | [Extracting Training Data from Large Language Models](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting) | USENIX Security Symposium 2022 |  |  |  |  |

### 6. Differential Privacy

*Awesome ML Differential Privacy Deep-dive: [Github](https://github.com/Billy1900/Awesome-Differential-Privacy).

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Aaron Roth and Cynthia Dwork | [The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf) | University of Pennsylvania's Book |  |  |  |  |
Abadi et al. | [Deep Learning with Differential Privacy](https://dl.acm.org/doi/abs/10.1145/2976749.2978318?casa_token=04AV26T6VT4AAAAA:LjK85DNm0rJ-lk0S2VDwtmAnHuIBVyKMYeU89wucbz6abzM8tDygrgWjI1ZzdQNUEvcKpW4nEMeb) | ACM 2016 |  |  |  |  |
Papernot et al. | [Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data](https://arxiv.org/abs/1610.05755) | ArXiv 2016 |  |  |  |  |
Papernot et al. | [Scalable Private Learning with PATE](https://arxiv.org/abs/1802.08908) | ArXiv 2018 |  |  |  |  |

### 7. Robust Mean Estimation in High Dimensions

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Zico Kolter and Aleksander Madry | [Adversarial Robustness - Theory and Practice](https://adversarial-ml-tutorial.org/) | NeurIPS 2018 |  |  |  |  |
Jerry Li | [Principled Approaches to Robust Machine Learning and Beyond](https://jerryzli.github.io/papers/phd-thesis.pdf) | MIT's Ph.D Thesis 2018 |  |  |  |  |
Jacob Steinhardt | [Robust Learning: Information Theory and Algorithms](https://www.stat.berkeley.edu/~jsteinhardt/pubs/steinhardt2018thesis.pdf) | Stanford's Ph.D Thesis 2018 |  |  |  |  |
Zhu et al. | [When does the Tukey Median work?](https://ieeexplore.ieee.org/abstract/document/9173995?casa_token=xAEyUler0lUAAAAA:c5jKyb5HnzURk8GKGU66h4jg_L_Az8ZwVg7lpxWd6mtrfnwGrASSqhNLvkMQ00WIdC0CqI0_-g) | IEEE 2020 |  |  |  |  |
Diakonikolas et al. | [Sever: A Robust Meta-Algorithm for Stochastic Optimization](https://proceedings.mlr.press/v97/diakonikolas19a.html) | PMLR 2019 |  |  |  |  |
Tran et al. | [Spectral Signatures in Backdoor Attacks](https://proceedings.neurips.cc/paper/2018/hash/280cf18baf4311c92aa5a042336587d3-Abstract.html) | NeurIPS 2018 |  |  |  |  |
Abbe et al. | [Poly-time universality and limitations of deep learning](https://arxiv.org/abs/2001.02992) | ArXiv 2020 |  |  |  |  |

### 8. Evasion Attacks & Defenses

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Akhtar et al. | [Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey](https://ieeexplore.ieee.org/abstract/document/8294186) | IEEE 2018 |  |  |  |  |
Athalye et al. | [Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](http://proceedings.mlr.press/v80/athalye18a.html) | PMLR 2018 |  |  |  |  |
Dhillon et al. | [Stochastic Activation Pruning for Robust Adversarial Defense](https://arxiv.org/abs/1803.01442) | ArXiv 2018 |  |  |  |  |
Schmidt et al. | [Adversarially Robust Generalization Requires More Data](https://proceedings.neurips.cc/paper/2018/hash/f708f064faaf32a43e4d3c784e6af9ea-Abstract.html) | NeurIPS 2018 |  |  |  |  |
Yin et al. | [Rademacher Complexity for Adversarially Robust Generalization](http://proceedings.mlr.press/v97/yin19b.html) | PMLR 2019 |  |  |  |  |
Tsipras et al. | [Robustness May Be at Odds with Accuracy](https://arxiv.org/abs/1805.12152) | ArXiv 2018 |  |  |  |  |

### 9. Test-Time Defenses

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |
 |  |  |  |  |  |  |
 |  |  |  |  |  |  |
 |  |  |  |  |  |  |

### 10. Certified Defenses

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Lecuyer et al. | [Certified Robustness to Adversarial Examples with Differential Privacy](https://ieeexplore.ieee.org/abstract/document/8835364?casa_token=BgsxdAlduEgAAAAA:cQdNBInOvS-ZeMZcadgtG1e0pArr6R-OWBnX0aeyuRMbqjMBIvNuCnAcnBaRjnBybl-TEN86WA) | IEEE 2019 |  |  |  |  |
Li et al. | [Certified Adversarial Robustness with Additive Noise](https://proceedings.neurips.cc/paper/2019/hash/335cd1b90bfa4ee70b39d08a4ae0cf2d-Abstract.html) | NeurIPS 2019 |  |  |  |  |

### 11. Federated Learning

*Awesome ML Federated Learning Deep-dive: [Github](https://github.com/poga/awesome-federated-learning).

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
McMahan et al. | [Communication-Efficient Learning of Deep Networks from Decentralized Data](http://proceedings.mlr.press/v54/mcmahan17a.html) | PMLR 2017 |  |  |  |  |
Konečný et al. | [Federated Optimization:Distributed Optimization Beyond the Datacenter](https://arxiv.org/abs/1511.03575) | ArXiv 2015 |  |  |  |  |
Stich et al. | [Local SGD Converges Fast and Communicates Little](https://arxiv.org/abs/1805.09767) | ICLR 2018 |  |  |  |  |
Li et al. | [On the Convergence of FedAvg on Non-IID Data](https://arxiv.org/abs/1907.02189) | ICLR 2020 |  |  |  |  |
Shamir et al. | [Communication-Efficient Distributed Optimization using an Approximate Newton-type Method](https://proceedings.mlr.press/v32/shamir14.html) | PMLR 2014 |  |  |  |  |
Wang et al. | [GIANT: Globally Improved Approximate Newton Method for Distributed Optimization](https://proceedings.neurips.cc/paper/2018/hash/dabd8d2ce74e782c65a973ef76fd540b-Abstract.html) | NeurIPS 2018 |  |  |  |  |
Mahajan et al. | [An efficient distributed learning algorithm based on effective local functional approximations](https://arxiv.org/abs/1310.8418) | ArXiv 2013 |  |  |  |  |
Smith et al. | [CoCoA: A General Framework for Communication-Efficient Distributed Optimization](https://www.jmlr.org/papers/volume18/16-512/16-512.pdf) | JMLR 2018 |  |  |  |  |
Zhang et al. | [Communication-Efficient Algorithms for Statistical Optimization](https://proceedings.neurips.cc/paper/2012/hash/e7f8a7fb0b77bcb3b283af5be021448f-Abstract.html) | NeurIPS 2013 |  |  |  |  |
Geiping et al. | [Inverting Gradients - How easy is it to break privacy in federated learning?](https://proceedings.neurips.cc/paper/2020/hash/c4ede56bbd98819ae6112b20ac6bf145-Abstract.html) | NeurIPS 2020 |  |  |  |  |
Agarwal et al. | [cpSGD: Communication-efficient and differentially-private distributed SGD](https://proceedings.neurips.cc/paper/2018/hash/21ce689121e39821d07d04faab328370-Abstract.html) | NeurIPS 2018 |  |  |  |  |
Bagdasaryan et al. | [How To Backdoor Federated Learning](https://proceedings.mlr.press/v108/bagdasaryan20a.html) | PMLR 2020 |  |  |  |  |
Bhagoji et al. | [Analyzing Federated Learning through an Adversarial Lens](https://proceedings.mlr.press/v97/bhagoji19a.html) | PMLR 2019 |  |  |  |  |
Tolpegin et al. | [Data Poisoning Attacks Against Federated Learning Systems](https://link.springer.com/chapter/10.1007/978-3-030-58951-6_24) | Springer 2020 |  |  |  |  |
Sun et al. | [Data Poisoning Attacks on Federated Machine Learning](https://ieeexplore.ieee.org/abstract/document/9618642?casa_token=nLXXx0riOtoAAAAA:gBTZfA-5ZvPikVGI-ksdtlYAbT3d2-WzCeyNTB086f_jHqV0rJVU9l2NrJPFJeUDltA2XbZzFw) | IEEE 2021 |  |  |  |  |
Fung et al. | [Mitigating Sybils in Federated Learning Poisoning](https://arxiv.org/abs/1808.04866) | ArXiv 2018 |  |  |  |  |
Li et al. | [Learning to Detect Malicious Clients for Robust Federated Learning](https://arxiv.org/abs/2002.00211) | ArXiv 2020 |  |  |  |  |
Blanchard et al. | [Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent](https://proceedings.neurips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html) | NeurIPS 2017 |  |  |  |  |
Mhamdi et al. | [The Hidden Vulnerability of Distributed Learning in Byzantium](https://proceedings.mlr.press/v80/mhamdi18a.html) | PMLR 2018 |  |  |  |  |
Yin et al. | [Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates](https://proceedings.mlr.press/v80/yin18a) | PMLR 2018 |  |  |  |  |
Chen et al. | [Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent](https://arxiv.org/abs/1705.05491) | ArXiv 2017 |  |  |  |  |
Fu et al. | [Attack-Resistant Federated Learning with Residual-based Reweighting](https://arxiv.org/abs/1912.11464) | ArXiv 2019 |  |  |  |  |
Sun et al. | [Can You Really Backdoor Federated Learning?](https://arxiv.org/abs/1911.07963) | ArXiv 2019 |  |  |  |  |
Andreina et al. | [BaFFLe: Backdoor Detection via Feedback-based Federated Learning](https://ieeexplore.ieee.org/abstract/document/9546463?casa_token=rTpaToMQfwQAAAAA:_w-HPkRDko2IciGmQM4XB31OVqgT-6mAYV6znvDhYZVZwpQYCg2333nCrqrV5WyMQ7KmeMV5rQ) | IEEE 2021 |  |  |  |  |
Wu et al. | [Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2011.01767) | ArXiv 2021 |  |  |  |  |

### 12. Uncertainty Calibration

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |
 |  |  |  |  |  |  |
 |  |  |  |  |  |  |
 |  |  |  |  |  |  |
 |  |  |  |  |  |  |
 |  |  |  |  |  |  |

### 13. AI Assurance

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
Batarseh et al. | [A survey on artificial intelligence assurance](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00445-7) | Springer 2021 |  |  |  |  |


## :four_leaf_clover: Statistical Methods & Data Analytics 
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Representative Learning
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Sequential Learning
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Reinforcement Learning
Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## :four_leaf_clover: Software Engineering

Author | Title | Publication | Code | Slide | Attempt 1 | Attempt 2 |
--- | --- | --- | --- |--- |--- |--- |
 |  |  |  |  |  |  |

## References

