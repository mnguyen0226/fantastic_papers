# Lists of Trustworthy ML Techniques

## 1. Poisoning Attacks
- "Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks" - Shafahi et al. NeurIPS 2018. 
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/Poison%20Frogs!%20Targeted%20Clean-Label%20Poisoning%20Attacks%20on%20Neural%20Networks.pdf)
    - [Code](https://github.com/ashafahi/inceptionv3-transferLearn-poison)
- "Understanding Black-box Predictions via Influence Functions" - Koh et al. ICML 2017. 
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/Understanding%20Black-box%20Predictions%20via%20Influence%20Functions.pdf)
    - [Code 1](https://worksheets.codalab.org/worksheets/0x2b314dc3536b482dbba02783a24719fd/), [Code 2](https://worksheets.codalab.org/worksheets/0x2b314dc3536b482dbba02783a24719fd/)

## 2. Backdoor Attacks
- "BadNets: Identifying Vulnerabilities in Machine Learning Model Supply Chain" - Gu et al.
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/BadNets:%20Identifying%20Vulnerabilities%20in%20the%20Machine%20Learning%20Model%20Supply%20Chain.pdf)
    - Code
- "Rethinking the Backdoor Attacks's Triggers: A Frequency Perspective" - Zeng et al. ICCV 2021
    - [Paper](https://github.com/mnguyen0226/trustworthy_ml_techniques/blob/main/docs/papers/Rethinking%20the%20Backdoor%20Attacks%E2%80%99%20Triggers:%20A%20Frequency%20Perspective.pdf)
    - Code
- "Transferable Clean-Label Poisoning Attacks on Deep Neural Nets" - Zhu et al. 2019
    - Paper
    - Code
- "Poisoning Attacks against Support Vector Machines" - Biggio et al. 2013
    - Paper
    - Code
- "Efficient Label Contamination Attacks Against Black-Box Learning Models" - Zhao et al. 2017
    - Paper
    - Code


## 3. Training-time Defense
- "Deep Partition Aggregation: Provable Defense against General Poisoning Attacks" - Levine et al. 2021
    - Paper
    - Code
- "DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations" - Borgnia et al. 2021
    - Paper
    - Code
- "On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping" - Hong et al. 2020
    - Paper
    - Code

## 4. Data Quality

## 5. Data Valuation

## 6. Privacy Attacks

## 7. Differential Privacy

## 8. Differentially Private Learning 

## 9. Robust Estimation

## 10. Evasion Attack

## 11. Test-time Defenses

## 12. Ceertified Defense

## 13. Federated Learning

## 14. Federated Learning Security

## 15. Federated Learning Privacy

## 16. Uncertainty Calibration